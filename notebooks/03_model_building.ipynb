{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Forecasting - Model Building\n",
    "## Time Series Model Development and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series specific\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "data_path = \"../data/processed/quarterly_mortgage_volume.csv\"\n",
    "quarterly_data = pd.read_csv(data_path, parse_dates=['date'])\n",
    "\n",
    "print(\"Data Overview:\")\n",
    "print(f\"Time period: {quarterly_data['date'].min()} to {quarterly_data['date'].max()}\")\n",
    "print(f\"Total quarters: {len(quarterly_data)}\")\n",
    "\n",
    "# Prepare the time series\n",
    "ts_data = quarterly_data.set_index('date')['total_loan_volume']\n",
    "ts_data = ts_data.asfreq('Q')\n",
    "\n",
    "# Plot the full series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts_data.index, ts_data.values / 1e6, linewidth=2)\n",
    "plt.title('Full Mortgage Origination Volume Time Series', fontweight='bold')\n",
    "plt.ylabel('Volume (Millions $)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_timeseries(data, test_size=0.2):\n",
    "    \"\"\"Split time series into train and test sets\"\"\"\n",
    "    n_test = int(len(data) * test_size)\n",
    "    \n",
    "    train = data.iloc[:-n_test]\n",
    "    test = data.iloc[-n_test:]\n",
    "    \n",
    "    print(f\"Training set: {train.index.min()} to {train.index.max()} ({len(train)} quarters)\")\n",
    "    print(f\"Test set: {test.index.min()} to {test.index.max()} ({len(test)} quarters)\")\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data = train_test_split_timeseries(ts_data, test_size=0.2)\n",
    "\n",
    "# Plot train-test split\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data.index, train_data.values / 1e6, label='Training', linewidth=2)\n",
    "plt.plot(test_data.index, test_data.values / 1e6, label='Test', linewidth=2, color='red')\n",
    "plt.title('Train-Test Split', fontweight='bold')\n",
    "plt.ylabel('Volume (Millions $)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sarima_model(train_data, seasonal_period=4):\n",
    "    \"\"\"Build SARIMA model using auto_arima for parameter selection\"\"\"\n",
    "    print(\"Building SARIMA model...\")\n",
    "    \n",
    "    # Use auto_arima to find best parameters\n",
    "    model = auto_arima(\n",
    "        train_data,\n",
    "        seasonal=True,\n",
    "        m=seasonal_period,  # quarterly data\n",
    "        stepwise=True,\n",
    "        suppress_warnings=True,\n",
    "        error_action='ignore',\n",
    "        trace=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBest SARIMA parameters: {model.order} {model.seasonal_order}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build SARIMA model\n",
    "sarima_model = build_sarima_model(train_data)\n",
    "\n",
    "# Generate forecasts\n",
    "sarima_forecast = sarima_model.predict(n_periods=len(test_data))\n",
    "sarima_forecast_index = pd.date_range(\n",
    "    start=test_data.index[0], \n",
    "    periods=len(test_data), \n",
    "    freq='Q'\n",
    ")\n",
    "sarima_forecast = pd.Series(sarima_forecast, index=sarima_forecast_index)\n",
    "\n",
    "print(f\"SARIMA forecast for test period complete\")\n",
    "print(f\"Forecast range: {sarima_forecast.index.min()} to {sarima_forecast.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Facebook Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prophet_model(train_data):\n",
    "    \"\"\"Build Facebook Prophet model\"\"\"\n",
    "    print(\"Building Prophet model...\")\n",
    "    \n",
    "    # Prepare data for Prophet\n",
    "    prophet_train = train_data.reset_index()\n",
    "    prophet_train.columns = ['ds', 'y']\n",
    "    \n",
    "    # Create and fit model\n",
    "    model = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        seasonality_mode='multiplicative',\n",
    "        changepoint_prior_scale=0.05,\n",
    "        seasonality_prior_scale=10\n",
    "    )\n",
    "    \n",
    "    model.fit(prophet_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build Prophet model\n",
    "prophet_model = build_prophet_model(train_data)\n",
    "\n",
    "# Generate forecasts\n",
    "future = prophet_model.make_future_dataframe(periods=len(test_data), freq='Q')\n",
    "prophet_forecast = prophet_model.predict(future)\n",
    "\n",
    "# Extract the forecast for test period\n",
    "prophet_test_forecast = prophet_forecast[prophet_forecast['ds'].isin(test_data.index)]\n",
    "prophet_forecast_series = pd.Series(\n",
    "    prophet_test_forecast['yhat'].values, \n",
    "    index=prophet_test_forecast['ds']\n",
    ")\n",
    "\n",
    "print(f\"Prophet forecast for test period complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exponential Smoothing (ETS) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ets_model(train_data, seasonal_period=4):\n",
    "    \"\"\"Build Exponential Smoothing model\"\"\"\n",
    "    print(\"Building Exponential Smoothing model...\")\n",
    "    \n",
    "    # Try different configurations\n",
    "    best_aic = np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    # Test additive vs multiplicative seasonality\n",
    "    for seasonal in ['add', 'mul']:\n",
    "        try:\n",
    "            model = ExponentialSmoothing(\n",
    "                train_data,\n",
    "                seasonal_periods=seasonal_period,\n",
    "                trend='add',\n",
    "                seasonal=seasonal\n",
    "            ).fit()\n",
    "            \n",
    "            if model.aic < best_aic:\n",
    "                best_aic = model.aic\n",
    "                best_model = model\n",
    "                best_seasonal = seasonal\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if best_model is not None:\n",
    "        print(f\"Best ETS model: Additive trend with {best_seasonal} seasonality\")\n",
    "        print(f\"AIC: {best_aic:.2f}\")\n",
    "    else:\n",
    "        # Fallback to simple model\n",
    "        print(\"Using fallback ETS model\")\n",
    "        best_model = ExponentialSmoothing(train_data).fit()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Build ETS model\n",
    "ets_model = build_ets_model(train_data)\n",
    "\n",
    "# Generate forecasts\n",
    "ets_forecast = ets_model.forecast(len(test_data))\n",
    "ets_forecast = pd.Series(ets_forecast, index=test_data.index)\n",
    "\n",
    "print(f\"ETS forecast for test period complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_models(train_data, test_data):\n",
    "    \"\"\"Build simple baseline models for comparison\"\"\"\n",
    "    baselines = {}\n",
    "    \n",
    "    # 1. Naive forecast (last value)\n",
    "    last_value = train_data.iloc[-1]\n",
    "    baselines['naive'] = pd.Series([last_value] * len(test_data), index=test_data.index)\n",
    "    \n",
    "    # 2. Seasonal naive (last year same quarter)\n",
    "    seasonal_naive = []\n",
    "    for date in test_data.index:\n",
    "        # Get same quarter from previous year\n",
    "        prev_year_date = date - pd.DateOffset(years=1)\n",
    "        if prev_year_date in train_data.index:\n",
    "            seasonal_naive.append(train_data.loc[prev_year_date])\n",
    "        else:\n",
    "            # Fallback to naive if not available\n",
    "            seasonal_naive.append(last_value)\n",
    "    \n",
    "    baselines['seasonal_naive'] = pd.Series(seasonal_naive, index=test_data.index)\n",
    "    \n",
    "    # 3. Moving average (4-quarter)\n",
    "    if len(train_data) >= 4:\n",
    "        moving_avg = train_data.rolling(window=4).mean().iloc[-1]\n",
    "    else:\n",
    "        moving_avg = train_data.mean()\n",
    "    \n",
    "    baselines['moving_avg'] = pd.Series([moving_avg] * len(test_data), index=test_data.index)\n",
    "    \n",
    "    print(\"Baseline models created:\")\n",
    "    for name in baselines.keys():\n",
    "        print(f\"  ‚Ä¢ {name}\")\n",
    "    \n",
    "    return baselines\n",
    "\n",
    "# Build baseline models\n",
    "baseline_models = build_baseline_models(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(test_data, forecasts_dict):\n",
    "    \"\"\"Evaluate all models and return metrics\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, forecast in forecasts_dict.items():\n",
    "        # Ensure we only compare available test periods\n",
    "        common_index = test_data.index.intersection(forecast.index)\n",
    "        \n",
    "        if len(common_index) == 0:\n",
    "            print(f\"Warning: No common dates for {model_name}\")\n",
    "            continue\n",
    "            \n",
    "        actual = test_data.loc[common_index]\n",
    "        predicted = forecast.loc[common_index]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(actual, predicted)\n",
    "        rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "        mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'Accuracy': max(0, 100 - mape)  # Simplified accuracy\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Combine all forecasts\n",
    "all_forecasts = {\n",
    "    'SARIMA': sarima_forecast,\n",
    "    'Prophet': prophet_forecast_series,\n",
    "    'ETS': ets_forecast,\n",
    "    'Naive': baseline_models['naive'],\n",
    "    'Seasonal_Naive': baseline_models['seasonal_naive'],\n",
    "    'Moving_Avg': baseline_models['moving_avg']\n",
    "}\n",
    "\n",
    "# Evaluate all models\n",
    "model_evaluation = evaluate_models(test_data, all_forecasts)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(model_evaluation.round(3))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = model_evaluation['Accuracy'].idxmax()\n",
    "best_accuracy = model_evaluation.loc[best_model_name, 'Accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} ({best_accuracy:.2f}% accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization of Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(train_data, test_data, forecasts_dict, evaluation_df):\n",
    "    \"\"\"Create comprehensive model comparison plots\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: All forecasts vs actual\n",
    "    axes[0,0].plot(train_data.index, train_data.values / 1e6, \n",
    "                   label='Training', color='black', linewidth=2)\n",
    "    axes[0,0].plot(test_data.index, test_data.values / 1e6, \n",
    "                   label='Actual', color='blue', linewidth=3)\n",
    "    \n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown', 'pink']\n",
    "    for i, (model_name, forecast) in enumerate(forecasts_dict.items()):\n",
    "        if model_name in ['SARIMA', 'Prophet', 'ETS']:  # Only show main models for clarity\n",
    "            axes[0,0].plot(forecast.index, forecast.values / 1e6, \n",
    "                          label=model_name, linestyle='--', linewidth=2, color=colors[i])\n",
    "    \n",
    "    axes[0,0].set_title('Model Forecasts vs Actual', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Volume (Millions $)')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Accuracy comparison\n",
    "    accuracy_data = evaluation_df['Accuracy'].sort_values(ascending=True)\n",
    "    bars = axes[0,1].barh(range(len(accuracy_data)), accuracy_data.values, \n",
    "                         color=['red' if x < 85 else 'green' for x in accuracy_data.values])\n",
    "    axes[0,1].set_yticks(range(len(accuracy_data)))\n",
    "    axes[0,1].set_yticklabels(accuracy_data.index)\n",
    "    axes[0,1].set_title('Model Accuracy (%)', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Accuracy (%)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        axes[0,1].text(width + 1, bar.get_y() + bar.get_height()/2, \n",
    "                      f'{width:.1f}%', ha='left', va='center')\n",
    "    \n",
    "    # Plot 3: Error metrics comparison\n",
    "    error_metrics = evaluation_df[['MAE', 'RMSE']].copy()\n",
    "    error_metrics = error_metrics.div(error_metrics.max())  # Normalize for comparison\n",
    "    \n",
    "    x_pos = np.arange(len(error_metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1,0].bar(x_pos - width/2, error_metrics['MAE'], width, label='MAE (normalized)')\n",
    "    axes[1,0].bar(x_pos + width/2, error_metrics['RMSE'], width, label='RMSE (normalized)')\n",
    "    \n",
    "    axes[1,0].set_xticks(x_pos)\n",
    "    axes[1,0].set_xticklabels(error_metrics.index, rotation=45)\n",
    "    axes[1,0].set_title('Normalized Error Metrics', fontweight='bold')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Residual analysis for best model\n",
    "    best_model_name = evaluation_df['Accuracy'].idxmax()\n",
    "    best_forecast = forecasts_dict[best_model_name]\n",
    "    \n",
    "    common_index = test_data.index.intersection(best_forecast.index)\n",
    "    residuals = test_data.loc[common_index] - best_forecast.loc[common_index]\n",
    "    \n",
    "    axes[1,1].scatter(best_forecast.loc[common_index] / 1e6, residuals / 1e6, alpha=0.7)\n",
    "    axes[1,1].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[1,1].set_title(f'Residual Plot - {best_model_name}', fontweight='bold')\n",
    "    axes[1,1].set_xlabel('Predicted Values (Millions $)')\n",
    "    axes[1,1].set_ylabel('Residuals (Millions $)')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create comparison plots\n",
    "comparison_fig = plot_model_comparison(train_data, test_data, all_forecasts, model_evaluation)\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "comparison_fig.savefig('../outputs/model_comparison.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "# Save models\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save SARIMA model\n",
    "joblib.dump(sarima_model, models_dir / 'sarima_model.pkl')\n",
    "\n",
    "# Save Prophet model\n",
    "with open(models_dir / 'prophet_model.json', 'w') as f:\n",
    "    f.write(prophet_model.to_json())\n",
    "\n",
    "# Save ETS model\n",
    "joblib.dump(ets_model, models_dir / 'ets_model.pkl')\n",
    "\n",
    "# Save evaluation results\n",
    "model_evaluation.to_csv('../outputs/model_evaluation_results.csv')\n",
    "\n",
    "# Save forecasts\n",
    "forecasts_df = pd.DataFrame(all_forecasts)\n",
    "forecasts_df['Actual'] = test_data\n",
    "forecasts_df.to_csv('../outputs/model_forecasts.csv')\n",
    "\n",
    "print(\"Models and results saved:\")\n",
    "print(\"  ‚Ä¢ SARIMA model: models/sarima_model.pkl\")\n",
    "print(\"  ‚Ä¢ Prophet model: models/prophet_model.json\")\n",
    "print(\"  ‚Ä¢ ETS model: models/ets_model.pkl\")\n",
    "print(\"  ‚Ä¢ Evaluation results: outputs/model_evaluation_results.csv\")\n",
    "print(\"  ‚Ä¢ Forecast data: outputs/model_forecasts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MODEL BUILDING - KEY INSIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüèÜ BEST PERFORMING MODEL: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {best_accuracy:.2f}%\")\n",
    "print(f\"   ‚Ä¢ MAE: ${model_evaluation.loc[best_model_name, 'MAE']:,.0f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: ${model_evaluation.loc[best_model_name, 'RMSE']:,.0f}\")\n",
    "\n",
    "print(f\"\\nüìä MODEL RANKING:\")\n",
    "ranked_models = model_evaluation['Accuracy'].sort_values(ascending=False)\n",
    "for i, (model, accuracy) in enumerate(ranked_models.items(), 1):\n",
    "    print(f\"   {i}. {model}: {accuracy:.2f}%\")\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "if best_accuracy >= 90:\n",
    "    print(\"   ‚Ä¢ Excellent model performance achieved\")\n",
    "    print(\"   ‚Ä¢ Ready for production forecasting\")\n",
    "elif best_accuracy >= 80:\n",
    "    print(\"   ‚Ä¢ Good model performance\")\n",
    "    print(\"   ‚Ä¢ Consider feature engineering for improvement\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ Moderate performance - investigate data issues\")\n",
    "    print(\"   ‚Ä¢ Consider external variables or different approach\")\n",
    "\n",
    "print(f\"\\nüîÆ NEXT STEPS:\")\n",
    "print(\"   ‚Ä¢ Proceed to forecasting with the best model\")\n",
    "print(\"   ‚Ä¢ Generate future predictions\")\n",
    "print(\"   ‚Ä¢ Create deployment pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this model building notebook we:\n",
    "1. Prepared training and test datasets\n",
    "2. Built three advanced models: SARIMA, Prophet, and ETS\n",
    "3. Created baseline models for comparison\n",
    "4. Evaluated all models using multiple metrics\n",
    "5. Identified the best performing model\n",
    "6. Saved models and results for future use\n",
    "\n",
    "The best model will be used for final forecasting in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
